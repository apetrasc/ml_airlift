{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 5, 2500)\n",
      "(108, 5, 2500)\n",
      "torch.Size([108, 5, 2500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: 0.3898019005373705\n",
      "re solid: 0.5209147930145264\n",
      "coefficient void: 0.6817193824724055\n",
      "re void: 0.29079747200012207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0348, 0.1760]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0348, 0.1760], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: -0.1647485140448878\n",
      "re solid: 0.8101611733436584\n",
      "coefficient void: 0.7338635899323337\n",
      "re void: 0.3038124442100525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0051, 0.2444]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0051, 0.2444], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: -0.015631452861730805\n",
      "re solid: 0.8696227073669434\n",
      "coefficient void: 0.7720275006772881\n",
      "re void: 0.2535753548145294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0242, 0.2523]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0242, 0.2523], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: 0.22132511857652615\n",
      "re solid: 0.5660746693611145\n",
      "coefficient void: 0.7129242702730739\n",
      "re void: 0.31134292483329773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0281, 0.1539]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0281, 0.1539], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: 0.23885455872869604\n",
      "re solid: 1.5686722993850708\n",
      "coefficient void: 0.656042154025596\n",
      "re void: 0.2868686616420746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0089, 0.2178]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0089, 0.2178], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: 0.19696909068331328\n",
      "re solid: 0.7271865606307983\n",
      "coefficient void: 0.743852703065823\n",
      "re void: 0.226871058344841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0251, 0.2367]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0251, 0.2367], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: 0.15687694965003732\n",
      "re solid: 0.4915516674518585\n",
      "coefficient void: 0.7183595698598413\n",
      "re void: 0.23903363943099976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0030, 0.2215]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0030, 0.2215], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: -0.38038313948762364\n",
      "re solid: 0.7907440662384033\n",
      "coefficient void: 0.6893273298805247\n",
      "re void: 0.24772949516773224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0322, 0.2563]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0322, 0.2563], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: 0.2527025458192605\n",
      "re solid: 0.5975217223167419\n",
      "coefficient void: 0.8299494530840292\n",
      "re void: 0.27793341875076294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0339, 0.2810]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0339, 0.2810], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_309389/1875637137.py:196: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 2)\n",
      "coefficient solid: -0.28130884195746597\n",
      "re solid: 0.7475320100784302\n",
      "coefficient void: 0.8132099468736793\n",
      "re void: 0.30799031257629395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/Document/yyamaguchi/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0035, 0.2775]], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "tensor([0.0035, 0.2775], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1, 4, 2500])\n",
      "torch.Size([1, 4, 2500])\n",
      "(5, 2500)\n",
      "res_list:[0.5209148  0.8101612  0.8696227  0.56607467 1.5686723  0.72718656\n",
      " 0.49155167 0.79074407 0.5975217  0.747532  ]\n",
      "coefs_list:[ 0.3898019  -0.16474851 -0.01563145  0.22132512  0.23885456  0.19696909\n",
      "  0.15687695 -0.38038314  0.25270255 -0.28130884]\n",
      "reg_list:[0.29079747 0.30381244 0.25357535 0.31134292 0.28686866 0.22687106\n",
      " 0.23903364 0.2477295  0.27793342 0.3079903 ]\n",
      "coefg_list:[0.68171938 0.73386359 0.7720275  0.71292427 0.65604215 0.7438527\n",
      " 0.71835957 0.68932733 0.82994945 0.81320995]\n",
      "res:0.7689981460571289\n",
      "coefs:0.06144582156434956\n",
      "reg:0.2745954692363739\n",
      "coefg:0.7351275900144595\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import os\n",
    "\n",
    "x_train=np.load(\"/mnt/sdb/yyamaguchi/simulationB4/bubble_glass_rand/dataset/x_train_five_env_std.npy\")\n",
    "t_train=np.load(\"/mnt/sdb/yyamaguchi/simulationB4/bubble_glass_rand/dataset/t_train_five_env_std.npy\")\n",
    "bdir=\"/mnt/sdb/yyamaguchi/simulationB4/bubble_glass_rand/dataset\"\n",
    "odir=os.path.join(bdir,\"outputs_env_std\")\n",
    "wdir=os.path.join(odir,\"weights\")\n",
    "ldir=os.path.join(odir,\"logs\")\n",
    "device=\"cuda:0\"\n",
    "B=10\n",
    "W=2500\n",
    "lr=0.001\n",
    "N=100\n",
    "C=5\n",
    "l1=1e-7\n",
    "l2=1e-7\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_length, input_channel):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_channel, 16, kernel_size=201, padding=100)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(16, 4, kernel_size=201, padding=100)\n",
    "        self.bn2 = nn.BatchNorm1d(4)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(4, 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.ln1 = nn.LayerNorm([16, input_length])\n",
    "        self.ln2 = nn.LayerNorm([4, input_length+2])\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        # x = self.ln1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        # x = self.ln2(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze(1)\n",
    "\n",
    "class GradCAM1d:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.hook_handles = []\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0].detach()\n",
    "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
    "        self.hook_handles.append(self.target_layer.register_backward_hook(backward_hook))\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        self.model.eval()\n",
    "        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n",
    "        self.model.zero_grad()\n",
    "        out = self.model(input_tensor)\n",
    "        print(out)\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            out = out[0]\n",
    "        target = out.squeeze()\n",
    "        print(target)\n",
    "        if target.ndim > 0:\n",
    "            target = target.sum()\n",
    "        self.model.zero_grad()\n",
    "        target.backward(retain_graph=True)\n",
    "        gradients = self.gradients         # [B, C, L]\n",
    "        print(gradients.shape)\n",
    "        activations = self.activations     # [B, C, L]\n",
    "        print(activations.shape)\n",
    "        weights = gradients.mean(dim=2, keepdim=True)  # [B, C, 1]\n",
    "        grad_cam_map = (weights * activations).sum(dim=1, keepdim=True)  # (B,1,L)\n",
    "        grad_cam_map = torch.relu(grad_cam_map)\n",
    "        grad_cam_map = torch.nn.functional.interpolate(\n",
    "            grad_cam_map, size=input_tensor.shape[2], mode='linear', align_corners=False\n",
    "        )\n",
    "        grad_cam_map = grad_cam_map.squeeze().cpu().numpy()\n",
    "        grad_cam_map = (grad_cam_map - grad_cam_map.min()) / (grad_cam_map.max() - grad_cam_map.min() + 1e-8)\n",
    "        return grad_cam_map\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "\n",
    "print(x_train.shape)\n",
    "# x_train=x_train.reshape(x_train.shape[0],-1)\n",
    "# x_train=x_train[:,[1,3,4],:]\n",
    "print(x_train.shape)\n",
    "x=torch.from_numpy(x_train).float()\n",
    "t=torch.from_numpy(t_train).float()\n",
    "# x=x.unsqueeze(1)\n",
    "print(x.shape)\n",
    "\n",
    "res_list=[]\n",
    "coefs_list=[]\n",
    "reg_list=[]\n",
    "coefg_list=[]\n",
    "xgradcam=x[50,:,:]\n",
    "xgradcam=xgradcam.unsqueeze(0)\n",
    "dataset=TensorDataset(x,t)\n",
    "Nttl=len(dataset)\n",
    "trasize=int(0.7*Nttl)\n",
    "valsize=int(0.15*Nttl)\n",
    "testsize=Nttl-trasize-valsize\n",
    "trainsetall,testset=random_split(dataset,[trasize+valsize,testsize])\n",
    "testloader=DataLoader(testset,batch_size=1,shuffle=False)\n",
    "\n",
    "for k in range(10):\n",
    "    trainset,valset=random_split(trainsetall,[trasize,valsize])\n",
    "    trainloader=DataLoader(trainset,batch_size=B,shuffle=True,drop_last=True)\n",
    "    valloader=DataLoader(valset,batch_size=B,shuffle=False)\n",
    "    model=SimpleCNN(W,C).to(device)\n",
    "\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer=optim.Adam(params=model.parameters(),lr=lr)\n",
    "    tlosshistory=[]\n",
    "    vlosshistory=[]\n",
    "\n",
    "    for epoch in range(N):\n",
    "        model.train()\n",
    "        rloss=0.0\n",
    "        for bx, by in trainloader:\n",
    "            bx=bx.to(device)\n",
    "            by=by.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            o=model(bx)\n",
    "            loss=criterion(o,by)\n",
    "            l1n=sum(p.abs().sum() for p in model.parameters())\n",
    "            l2n=sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss=loss+l1*l1n+l2*l2n\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            rloss=rloss+loss.item()*bx.size(0)\n",
    "        eloss=rloss/len(trainloader.dataset)\n",
    "        tlosshistory.append(eloss)\n",
    "\n",
    "        model.eval()\n",
    "        vrloss=0.0\n",
    "        with torch.no_grad():\n",
    "            for vx,vy in valloader:\n",
    "                vx=vx.to(device)\n",
    "                vy=vy.to(device)\n",
    "                vo=model(vx)\n",
    "                vloss=criterion(vo,vy)\n",
    "                vrloss+=vloss.item()*vx.size(0)\n",
    "        veloss=vrloss/len(valloader.dataset)\n",
    "        vlosshistory.append(veloss)\n",
    "\n",
    "    os.makedirs(odir,exist_ok=True)\n",
    "    os.makedirs(wdir,exist_ok=True)\n",
    "    os.makedirs(ldir,exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(wdir,f'model{k}.pth'))\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, N + 1), [np.log(l) for l in tlosshistory], label='Train Log(Loss)')\n",
    "    plt.plot(range(1, N + 1), [np.log(l) for l in vlosshistory], label='Validation Log(Loss)')\n",
    "    plt.title('Learning Curve (Log Loss)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Log(Loss)')\n",
    "    plt.rcParams[\"font.size\"] = 16\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ldir, f'learning_curve_log{k}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, N + 1), tlosshistory, label='Train Loss')\n",
    "    plt.plot(range(1, N + 1), vlosshistory, label='Validation Loss')\n",
    "    plt.title('Learning Curve (Loss)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.rcParams[\"font.size\"] = 20\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ldir, f'learning_curve{k}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    model.load_state_dict(torch.load(os.path.join(wdir,f'model{k}.pth')))\n",
    "    model=model.to(device)\n",
    "    model.eval()\n",
    "    pred=[]\n",
    "    targ=[]\n",
    "    with torch.no_grad():\n",
    "        for tx,ty in testloader:\n",
    "            tx=tx.to(device)\n",
    "            ty=ty.to(device)\n",
    "            o=model(tx)\n",
    "            pred.append(o.cpu())\n",
    "            targ.append(ty.cpu())\n",
    "    pred=torch.cat(pred,dim=0)\n",
    "    targ=torch.cat(targ,dim=0)\n",
    "\n",
    "    pred=pred.numpy()\n",
    "    targ=targ.numpy()\n",
    "\n",
    "    print(pred.shape)\n",
    "\n",
    "    corr=np.corrcoef(targ[:,0],pred[:,0])\n",
    "    coef=corr[0,1]\n",
    "    print(f'coefficient solid: {coef}')\n",
    "    re=np.mean(np.abs(targ[:,0]-pred[:,0])/(targ[:,0]+1e-7))\n",
    "    print(f're solid: {re}')\n",
    "    res_list.append(re)\n",
    "    coefs_list.append(coef)\n",
    "    corr=np.corrcoef(targ[:,1],pred[:,1])\n",
    "    coef=corr[0,1]\n",
    "    print(f'coefficient void: {coef}')\n",
    "    re=np.mean(np.abs(targ[:,1]-pred[:,1])/(targ[:,1]+1e-7))\n",
    "    print(f're void: {re}')\n",
    "    reg_list.append(re)\n",
    "    coefg_list.append(coef)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(targ[:,0], pred[:,0], alpha=0.6, marker='o', label='Solid fraction')\n",
    "    plt.plot([-1, 1], [-1, 1], 'r--', label='Ideal (y=x)')\n",
    "    plt.title('Predicted vs Actual Values')\n",
    "    plt.xlabel('Actual Value')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    plt.xlim(-0.0, 0.05)\n",
    "    plt.ylim(-0.0, 0.05)\n",
    "    plt.rcParams[\"font.size\"] = 20\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ldir, f'val_pred_vs_actual_solid{k}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(targ[:,1], pred[:,1], alpha=0.6, marker='o', label='Void fraction')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Ideal (y=x)')\n",
    "    plt.title('Predicted vs Actual Values')\n",
    "    plt.xlabel('Actual Value')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    plt.xlim(0, 0.4)\n",
    "    plt.ylim(0, 0.4)\n",
    "    plt.rcParams[\"font.size\"] = 20\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ldir, f'val_pred_vs_actual_void{k}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    target_layer = None\n",
    "    if hasattr(model, 'conv3'):\n",
    "        target_layer = model.conv3\n",
    "    elif hasattr(model, 'layer3'):\n",
    "        target_layer = model.layer3\n",
    "    else:\n",
    "        for m in reversed(list(model.modules())):\n",
    "            if isinstance(m, torch.nn.Conv1d):\n",
    "                target_layer = m\n",
    "                break\n",
    "    if target_layer is None:\n",
    "        raise RuntimeError(\"Could not find a suitable layer for Grad-CAM.\")\n",
    "\n",
    "    gradcam_output_dir = os.path.join(odir, \"gradcam\")\n",
    "    gradcam = GradCAM1d(model, target_layer)\n",
    "    grad_cam_map = gradcam(xgradcam)\n",
    "    gradcam.remove_hooks()\n",
    "    grad_cam_map *= 60\n",
    "    grad_cam_map -= amp\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    amp=10\n",
    "    plt.plot(ta, xgradcamplt[0,:], label='TDX1')\n",
    "    plt.plot(ta, amp+xgradcamplt[1,:], label='TDX2')\n",
    "    plt.plot(ta, 2*amp+xgradcamplt[2,:], label='TDX3')\n",
    "    plt.plot(ta, 3*amp+xgradcamplt[3,:], label='TDX4')\n",
    "    plt.plot(ta, 4*amp+xgradcamplt[4,:], label='TDX5')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.plot(ta,grad_cam_map[:], color='red', label='Saliency-Map')\n",
    "    plt.fill_between(ta, grad_cam_map[:],-amp,\n",
    "            color='red',alpha=0.1)\n",
    "    plt.xlabel('Time Axis')\n",
    "    plt.ylabel('Pressure [-]')\n",
    "    plt.ylim(-amp,5*amp)\n",
    "    plt.yticks([-amp,0,amp,2*amp,3*amp,4*amp,5*amp],[f\"{-amp}\",\"0\",\"0\",\"0\",\"0\",\"0\",f\"{amp}\"])\n",
    "    plt.xlim(0,W)\n",
    "    plt.grid(True)\n",
    "    # plt.xlim(35,40)\n",
    "    plt.tight_layout()\n",
    "    new_save_path = os.path.join(odir, f\"gradcam{k}.png\")\n",
    "    plt.savefig(new_save_path)\n",
    "    plt.close()\n",
    "\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 1. まずデバイスに送る\n",
    "    xgradcam = xgradcam.to(device)\n",
    "\n",
    "    # 2. デバイス移動「後」に勾配追跡をONにする\n",
    "    xgradcam.requires_grad_()\n",
    "\n",
    "    # 3. 予測\n",
    "    output = model(xgradcam)\n",
    "\n",
    "    # 4. 逆伝播（クラス1に注目する場合）\n",
    "    loss = output[0, 1]\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. 勾配を取り出す（.gradプロパティに格納されている）\n",
    "    if xgradcam.grad is not None:\n",
    "        grads = xgradcam.grad.data # ここで [1, 5, 2500] が取得できる\n",
    "        \n",
    "        # --- チャネル別重要度スコアの計算 ---\n",
    "        # abs()をとって時間方向に平均\n",
    "        salimap = grads.abs().squeeze(0)\n",
    "        \n",
    "        # 0~1に正規化（安全のために 1e-8 を足す）\n",
    "        salimap = salimap / (salimap.max() + 1e-8)\n",
    "        \n",
    "        salimap = salimap.cpu().numpy()\n",
    "        salimap = salimap*10\n",
    "        print(salimap.shape)\n",
    "    else:\n",
    "        print(\"勾配が取得できませんでした。requires_gradの位置を確認してください。\")\n",
    "\n",
    "    ta=np.arange(W)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    amp=10\n",
    "    xgradcam=xgradcam.detach().cpu()\n",
    "    xgradcamplt=xgradcam.numpy().squeeze(0)\n",
    "    plt.plot(ta, xgradcamplt[0,:], label='TDX1')\n",
    "    plt.plot(ta, amp+xgradcamplt[1,:], label='TDX2')\n",
    "    plt.plot(ta, 2*amp+xgradcamplt[2,:], label='TDX3')\n",
    "    plt.plot(ta, 3*amp+xgradcamplt[3,:], label='TDX4')\n",
    "    plt.plot(ta, 4*amp+xgradcamplt[4,:], label='TDX5')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.plot(ta,salimap[0,:], color='red', label='Saliency-Map')\n",
    "    plt.fill_between(ta, salimap[0,:],0,\n",
    "            color='red',alpha=0.1)\n",
    "    plt.plot(ta,amp+salimap[1,:], color='red', label='Saliency-Map')\n",
    "    plt.fill_between(ta, amp+salimap[1,:],amp,\n",
    "            color='red',alpha=0.1)\n",
    "    plt.plot(ta,2*amp+salimap[2,:], color='red', label='Saliency-Map')\n",
    "    plt.fill_between(ta, 2*amp+salimap[2,:],2*amp,\n",
    "            color='red',alpha=0.1)\n",
    "    plt.plot(ta,3*amp+salimap[3,:], color='red', label='Saliency-Map')\n",
    "    plt.fill_between(ta, 3*amp+salimap[3,:],3*amp,\n",
    "            color='red',alpha=0.1)\n",
    "    plt.plot(ta,4*amp+salimap[4,:], color='red', label='Saliency-Map')\n",
    "    plt.fill_between(ta, 4*amp+salimap[4,:],4*amp,\n",
    "            color='red',alpha=0.1)\n",
    "    plt.xlabel('Time Axis')\n",
    "    plt.ylabel('Pressure [-]')\n",
    "    plt.ylim(-amp,5*amp)\n",
    "    plt.yticks([-amp,0,amp,2*amp,3*amp,4*amp,5*amp],[f\"{-amp}\",\"0\",\"0\",\"0\",\"0\",\"0\",f\"{amp}\"])\n",
    "    plt.xlim(0,W)\n",
    "    plt.grid(True)\n",
    "    # plt.xlim(35,40)\n",
    "    plt.tight_layout()\n",
    "    new_save_path = os.path.join(odir, f\"saliency{k}.png\")\n",
    "    plt.savefig(new_save_path)\n",
    "    plt.close()\n",
    "\n",
    "res_list=np.array(res_list)\n",
    "coefs_list=np.array(coefs_list)\n",
    "reg_list=np.array(reg_list)\n",
    "coefg_list=np.array(coefg_list)\n",
    "print(f\"res_list:{res_list}\")\n",
    "print(f\"coefs_list:{coefs_list}\")\n",
    "print(f\"reg_list:{reg_list}\")\n",
    "print(f\"coefg_list:{coefg_list}\")\n",
    "print(f\"res:{np.mean(res_list)}\")\n",
    "print(f\"coefs:{np.mean(coefs_list)}\")\n",
    "print(f\"reg:{np.mean(reg_list)}\")\n",
    "print(f\"coefg:{np.mean(coefg_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import os\n",
    "\n",
    "x_train=np.load(\"/mnt/sdb/yyamaguchi/simulationB4/bubble_glass_rand/dataset/x_train_five_env_std.npy\")\n",
    "t_train=np.load(\"/mnt/sdb/yyamaguchi/simulationB4/bubble_glass_rand/dataset/t_train_five_env_std.npy\")\n",
    "bdir=\"/mnt/sdb/yyamaguchi/simulationB4/bubble_glass_rand/dataset\"\n",
    "odir=os.path.join(bdir,\"outputs_env_std\")\n",
    "wdir=os.path.join(odir,\"weights\")\n",
    "ldir=os.path.join(odir,\"logs\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
